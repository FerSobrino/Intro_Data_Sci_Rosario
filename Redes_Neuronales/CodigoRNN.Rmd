---
title: "Redes Neuronales"
output: rmarkdown::html_vignette

---

```{r setup, include=FALSE}
library(tidyverse)
library(tensorflow)
library(keras)
knitr::opts_chunk$set(echo = TRUE)
```

# Clasificar datos escritos a mano 

#### Pre-requisitos: 

Vamos a necesitar la siguientes librerias. Ojo se instalan de manera distinta que la mayoría de los paquetes en r. (Recuerden también cargar tidyverse para la manipulación de los datos)

```{r, eval = FALSE, echo = TRUE, message=FALSE, warning=FALSE}
install.packages("tensorflow")
library(tensorflow)
install_tensorflow()
install.packages("keras")
library(keras)
install_keras()
```
Qué son estas dos librerias? 

* Tensorflow: libreria open source para modelos de ML [info](https://tensorflow.rstudio.com/)
* Keras: API de redes neuronales [info](https://keras.rstudio.com/)

#### Datos 
Los datos los pueden encontrar acá en formato .cvs [click aqui](https://pjreddie.com/projects/mnist-in-csv/). Useremos solo los datos de entrenamiento así que descarguen el train set. 

```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
raw_data <- read_csv("mnist_train.csv",col_names = FALSE)
dim(raw_data)
head(raw_data)
```
Esta base de datos contiene 60,000 ejemplos de números escritos a mano con 785 columnas. La primera columna es la clasificación del dígito un número entre 0 y 9. Las otras 784 columnas representa la escala de grises de cada uno de los pixeles en las imágenes. Las imagenes son de 28x28 y la escala de grises es un numero entre 0 y 1 donde 0 es que el pixel es todo blanco y 1 que es todo negro. 

```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
barplot(table(raw_data[,1]), col=rainbow(10, 0.5), main="n Digits in Train")
```

Hay alrededor de 5500 observaciones para cada uno de los dígitos. Para ver como se ven los dígitos escritos vamos a graficarlos 

```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
pixels_gathered <- raw_data %>%
  head(10000) %>%
  rename(label = X1) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance) %>%
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)
pixels_gathered %>%
  filter(instance <= 12) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ instance + label)
```

#### Preparar los datos

Vamos a dividir nuestros datos en 2 conjuntos distintos: entrenamiento y prueba. El conjunto de entrenamiento es el 80% del nuestra base original y el 20% restante es el conjunto de prueba. Vamos a dividir cada uno de estos conjuntos es y y x. Donde y contiene la clasificación del dígito y x es el valor para cada uno de los pixeles. 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
idx <- sample(seq(1, 2), size = nrow(raw_data), replace = TRUE, prob = c(.8, .2))
train <- raw_data[idx == 1,]
test <- raw_data[idx == 2,]
train_x <- train %>% select(-X1) %>% as.matrix()
train_y <- train %>% select(X1) %>% as.matrix()
test_x <- test %>% select(-X1) %>% as.matrix()
test_y <- test %>% select(X1) %>% as.matrix()
```

Necesitamos transformar las y en variables categóricas, en la base original la y es el dígito, acá transformamos ese dígito en un vector con un 1 en el valor del número. Un 1 es el  vector (0,1,0,0,0,0,0,0,0,0,0)^T
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
train_y <- to_categorical(train_y,10)
test_y <- to_categorical(test_y,10)
```


#### Definir y configurar el modelo 

Para definir el modelo usamos el paquete keras. Definimos una red neuronal secuencial con una capa de entrada con 754 nodos, una capa intermedia con 15 nodos cada una (como en las slides) 

```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model <- keras_model_sequential() 
model %>%
  layer_dense(units = 15, activation = "sigmoid", input_shape = c(784)) %>%
  layer_dense(units = 10, activation = "sigmoid")
summary(model)
```

Antes de entrenar el modelo tenemos que especificar la función de costos, la métrica para evaluar la red y el algoritmo para optimizar. En este caso utilizaremos cross-entropy, precisión y descenso de gradiente estocástico. 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = "sgd",
  metrics = c('accuracy')
)
```

#### Entrenar el modelo 

Utilizar fit para estimar los parámetros de la red. Aquí definimos el tamaño de los mini-batches y la cantidades de ephocs que queremos correr 

```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
fit_digits <- model %>% fit(
  train_x, train_y, 
  epochs = 30, 
  batch_size = 128,
  validation_split = 0.2
)
```

Ya que estimamos todos los parámetros podemos evaluar como lo hizo el modelo 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% evaluate(train_x, train_y)
model %>% evaluate(test_x, test_y)
plot(fit_digits)
```

Si queremos encontrar solo las y's estimadas basta con 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
y_hat <- model %>% predict(test_x)
head(y_hat)
```
Si queremos encontrar cual es el dígito que el modelo predice solo necesitamos manipular un poco este objeto. 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
y_hat <- as.data.frame(y_hat)
y_hat <- y_hat %>%
  mutate(digit = max.col(y_hat))
```




Este es el código básico, ahora podemos evaluar que pasa si cambiamos cosas como la cantidad de ephocs, agregamos métodos de regularización, agregamos nodos a nuestra capa intermedia o agregamos más capas. 


#### Cómo podemos mejorar nuestra red?  

##### Cambiando nuestra función de costos
Solo necesitamos cambiar la función y dejaremos la estructura de la red intacta. 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = "sgd",
  metrics = c('accuracy')
)
```
Evaluamos como lo hace con respecto a la primera: 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% fit(
  train_x, train_y, 
  epochs = 30, 
  batch_size = 128,
  validation_split = 0.2
)
model %>% evaluate(train_x, train_y)
model %>% evaluate(test_x, test_y)
```


##### Cambiando nuestra función de activación en la última capa 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model <- keras_model_sequential() 
model %>%
  layer_dense(units = 15, activation = "sigmoid", input_shape = c(784)) %>%
  layer_dense(units = 10, activation = "softmax")
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = "sgd",
  metrics = c('accuracy')
)
```
Evaluamos como lo hace con respecto a la primera:
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% fit(
  train_x, train_y, 
  epochs = 30, 
  batch_size = 128,
  validation_split = 0.2
)
model %>% evaluate(train_x, train_y)
model %>% evaluate(test_x, test_y)
```

##### Regularización 
Vamos a usar dropout en nuestra red original y ver si esto mejora la precisión
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model <- keras_model_sequential() 
model %>%
  layer_dense(units = 15, activation = "sigmoid", input_shape = c(784)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 10, activation = "sigmoid")
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = "sgd",
  metrics = c('accuracy')
)
```
Evaluamos como lo hace nuestra nueva red
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% fit(
  train_x, train_y, 
  epochs = 30, 
  batch_size = 128,
  validation_split = 0.2
)
model %>% evaluate(train_x, train_y)
model %>% evaluate(test_x, test_y)
```

##### Qué pasa si agregamos estos tres cambios juntos? 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model <- keras_model_sequential() 
model %>%
  layer_dense(units = 15, activation = "sigmoid", input_shape = c(784)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 10, activation = "softmax")
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = "sgd",
  metrics = c('accuracy')
)
```
Evaluamos como lo hace nuestra nueva red
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
fit_NN <- model %>% fit(
  train_x, train_y, 
  epochs = 30, 
  batch_size = 128,
  validation_split = 0.2
)
model %>% evaluate(train_x, train_y)
model %>% evaluate(test_x, test_y)
plot(fit_NN)
```


#### Deep Learning 

Hasta ahora nuestra red ha tenido una sola copa intermedia. Agregar más capas es muy sencillo solo necesitamos declararlas en el modelo original. 
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model <- keras_model_sequential() 
model %>%
  layer_dense(units = 15, activation = 'sigmoid', input_shape = c(784)) %>%
  layer_dropout(rate = 0.5) %>%
   layer_dense(units = 128, activation = 'sigmoid') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = "sgd",
  metrics = c('accuracy')
)
```
Evaluamos como lo hace nuestra nueva red
```{r, eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
model %>% fit(
  train_x, train_y, 
  epochs = 30, 
  batch_size = 128,
  validation_split = 0.2
)
model %>% evaluate(train_x, train_y)
model %>% evaluate(test_x, test_y)
```

###### CNNs
Podemos utilizar keras para evaluar redes mas complejas, por ejemplo una CNN. Vamos a modificar un poco nuestros datos para que el paquete pueda leerlos. 
La CNN en este caso va a conectar cada uno de los pixeles de la capa inicial, recordemos que nuestros datos son de 28x28 pixeles y hasta ahora los tratamos como un vector de 748

```{r,eval = TRUE, echo = TRUE}
train_x_cnn <- array_reshape(train_x, 
                               dim = c(nrow(train_x), 28, 28, 1)
                               )
test_x_cnn <- array_reshape(test_x, 
                               dim = c(nrow(test_x), 28, 28, 1)
                               )
```

Definimos nuestra red con una sola convolución que recorre regiones de 3x3 nodos, seguida de una capa de pooling seguida por una capa completamente conectada (como todas las que hemos usado hasta ahora) con 15 nodos, la última capa es la de salida, igual que en los casos anteriores es una capa densa con 10 nodos de salida. 

```{r,eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
modelcnn <- keras_model_sequential() %>%
   layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'sigmoid',
                input_shape = c(28, 28, 1))%>%
  layer_max_pooling_2d(pool_size = c(3,3)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 15, activation = 'sigmoid') %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 10, activation = 'sigmoid')
  
```


Definimos la función de costos, el método de optimización y la métrica. 
```{r,eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
modelcnn %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
```

Por último entrenamos el modelo:

```{r,eval = TRUE, echo = TRUE, message=FALSE, warning=FALSE}
CNN <- modelcnn %>% fit(
  train_x_cnn, train_y, 
  epochs = 5, 
  batch_size = 128,
  validation_split = 0.2
)
modelcnn %>% evaluate(train_x_cnn, train_y)
modelcnn %>% evaluate(test_x_cnn, test_y)
plot(CNN)
```



